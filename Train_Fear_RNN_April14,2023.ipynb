{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY1Uc6o2EeOldgIWCRgAUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Approach-Analytics/Anger-classifier/blob/main/Train_Fear_RNN_April14%2C2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Despair: Refactored training run - April 12,2023\n",
        "\n",
        "Synchronize this notebook with the new dataset creation capacity. "
      ],
      "metadata": {
        "id": "2aNbKs-0wBj0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf-ueAD8v7Td",
        "outputId": "d3d5667d-205a-4c42-ca71-c8eb5673452c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.9/dist-packages (from nlp) (9.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from nlp) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from nlp) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from nlp) (3.11.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from nlp) (2.27.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from nlp) (1.5.3)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->nlp) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->nlp) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, nlp\n",
            "Successfully installed dill-0.3.6 nlp-0.4.0 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "#Need pip install tensorflow with...Jupyter Notebook\n",
        "\n",
        "!pip install nlp\n",
        "#!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1JrYFgcnwaaJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dec 7, 2022: Not sure what this piece of code is used for... \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "F49j2PXxwdjv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.width', 200)\n",
        "pd.set_option('display.max_columns', 100)\n"
      ],
      "metadata": {
        "id": "50a7uMJE_1zJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing dataset and fear_axis definition "
      ],
      "metadata": {
        "id": "jlHF09wSweqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the despair axis\n",
        "#Removed: April 12, 2023: \"content\"\n",
        "\n",
        "#Defining the fear axis \n",
        "\n",
        "fear_axis= {\n",
        "    \"dread\" : ['panic','dread','horror','horrified','horrifying','terror','terrified','terrifying'],\n",
        "    \"fear\":  [\"fear\", \"fearful\", \"fright\",\"frightening\",\"afraid\",\"frightful\",\"frightfully\",\"frightened\",'scared','scary','scare'],\n",
        "    \"anxiety\": [\"anxious\",\"anxiety\",'angst','anxiousness'],\n",
        "    \"worry\":[\"worry\", \"worried\",\"worrying\",\"worries\"],\n",
        "    \"concern\": [\"concern\", \"concerning\",\"concerned\",\"concerns\"],\n",
        "    \"calm\": [\"calm\",\"peaceful\",\"serene\",\"serenity\",\"untroubled\", \"content\", \"contented\", \"composed\", \"tranquil\"]\n",
        "    }\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "0rAr8P5A4kZA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_4oC8eawtYt",
        "outputId": "c942efda-6b0b-41f0-f5af-5c24858c2945"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset \n",
        "\n",
        "path = \"/content/drive/MyDrive/Sean/Emoclass/Emotion datasets/Fear_G2000_266K_April14,2023.csv\"\n",
        "df=pd.read_csv(path)"
      ],
      "metadata": {
        "id": "nURRLtOYwv67"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "N7ho0hEJ6VVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LXmCKFFZ7DFl",
        "outputId": "0950a705-e81b-40ae-8dca-1b052013e15d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     emotion1 emotion2  word_count  Emotion_Word_Count\n",
              "0  when we entered any of the big restaurants i w...       fright     fear          84                   3\n",
              "1  when we entered any of the big restaurants i w...    frightful     fear          84                   3\n",
              "2  when we entered any of the big restaurants i w...  frightfully     fear          84                   3\n",
              "3  next morning at five we took the dogs and star...      concern  concern          82                   2\n",
              "4  even if she was very small and i was large and...      concern  concern          80                   2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66a9a1e9-ac2f-4cbd-bf1f-8aa46726c527\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>emotion1</th>\n",
              "      <th>emotion2</th>\n",
              "      <th>word_count</th>\n",
              "      <th>Emotion_Word_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when we entered any of the big restaurants i w...</td>\n",
              "      <td>fright</td>\n",
              "      <td>fear</td>\n",
              "      <td>84</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>when we entered any of the big restaurants i w...</td>\n",
              "      <td>frightful</td>\n",
              "      <td>fear</td>\n",
              "      <td>84</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>when we entered any of the big restaurants i w...</td>\n",
              "      <td>frightfully</td>\n",
              "      <td>fear</td>\n",
              "      <td>84</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>next morning at five we took the dogs and star...</td>\n",
              "      <td>concern</td>\n",
              "      <td>concern</td>\n",
              "      <td>82</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>even if she was very small and i was large and...</td>\n",
              "      <td>concern</td>\n",
              "      <td>concern</td>\n",
              "      <td>80</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66a9a1e9-ac2f-4cbd-bf1f-8aa46726c527')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-66a9a1e9-ac2f-4cbd-bf1f-8aa46726c527 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-66a9a1e9-ac2f-4cbd-bf1f-8aa46726c527');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAA_wiJS7L5M",
        "outputId": "8cf3f1dd-e4fc-48b9-86da-0393d244d397"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'emotion1', 'emotion2', 'word_count', 'Emotion_Word_Count'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Relabelling a column... if needed... \n",
        "\n",
        "df.rename(columns={\"emotion2\": \"label\"},inplace =True)"
      ],
      "metadata": {
        "id": "YWVVU1YDw4aU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see if we have missing values...No missing values... \n",
        "\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz0wm7a_w46o",
        "outputId": "4df25a70-bb7f-45f4-d289-aa3b9118863c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text                  0\n",
              "emotion1              0\n",
              "label                 0\n",
              "word_count            0\n",
              "Emotion_Word_Count    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Truncating Word Length "
      ],
      "metadata": {
        "id": "wpxXO76jzEPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining function to calculate word length\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "df['word_count'] = df['text'].apply(count_words)"
      ],
      "metadata": {
        "id": "Eh8bGSpyqDTT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_count'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENpPmMDCqJCd",
        "outputId": "2c80a443-d13e-4e2d-a22f-049ebe928f1b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    266262.000000\n",
              "mean         74.350933\n",
              "std           5.825781\n",
              "min          30.000000\n",
              "25%          71.000000\n",
              "50%          74.000000\n",
              "75%          78.000000\n",
              "max          99.000000\n",
              "Name: word_count, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_text(text, max_len, keyword):\n",
        "    if len(text) <= max_len:\n",
        "        return text\n",
        "    \n",
        "    # Find the index of the keyword in the text\n",
        "    keyword_idx = text.find(keyword)\n",
        "    \n",
        "    # If the keyword is not found or is too close to the ends of the text, just truncate from the ends\n",
        "    if keyword_idx == -1 or keyword_idx < max_len//2 or keyword_idx > len(text) - max_len//2:\n",
        "        return text[:max_len]\n",
        "    \n",
        "    # Truncate from both sides\n",
        "    start_idx = keyword_idx - max_len//2\n",
        "    end_idx = keyword_idx + max_len//2\n",
        "    return text[start_idx:end_idx]\n"
      ],
      "metadata": {
        "id": "FBISwaQKzH9D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to the 'text' column and create a new column 'truncated_text'\n",
        "df['truncated_text'] = df.apply(lambda row: truncate_text(row['text'], max_len=100, keyword='emotion1'), axis=1)"
      ],
      "metadata": {
        "id": "nf8p5Vkm-2_8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Redoing the word count....I think this needs to get reapplied... \n",
        "\n",
        "df[\"word_count_trunc\"] = df['truncated_text'].apply(count_words)"
      ],
      "metadata": {
        "id": "yHzy_cSZBq8D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking word length again... \n",
        "\n",
        "df['word_count_trunc'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSKkAJsp_InD",
        "outputId": "63b051d9-b276-4be2-8db4-9ef91a7d5d80"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    266262.000000\n",
              "mean         19.404132\n",
              "std           2.065458\n",
              "min           7.000000\n",
              "25%          18.000000\n",
              "50%          19.000000\n",
              "75%          21.000000\n",
              "max          35.000000\n",
              "Name: word_count_trunc, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Replacing the emotion words in the dataset "
      ],
      "metadata": {
        "id": "6ibTfjthxBav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate all the unique emotion words that then get replaced... \n",
        "\n",
        "a = df['emotion1'].unique().tolist()\n",
        "\n",
        "#The list a is our list of variable responses from the dataset... \n",
        "keyword = \"emotion word\"\n",
        "words = a\n",
        "for j in words: \n",
        "  df['text'] = df['text'].str.replace(j,keyword)"
      ],
      "metadata": {
        "id": "L_g8yc8uxFT6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Counting vocab size "
      ],
      "metadata": {
        "id": "-M9BLAlNDVSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Total vocabulary size prior to truncation \n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# convert the text into a list\n",
        "text = df['text'].tolist()\n",
        "\n",
        "# create a Tokenizer object\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# fit the tokenizer on the text data\n",
        "tokenizer.fit_on_texts(text)\n",
        "\n",
        "# calculate the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "BaGR5rfwDX4F"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36WeYWzXDZU2",
        "outputId": "e4060b9d-fd1b-4760-f589-c9e68f8464e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128945"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting into train, validate and test datasets inluding shuffle dataset "
      ],
      "metadata": {
        "id": "oyEbG1u2zkZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This outputs 3 different dataframes... originally 0.6 and 0.8\n",
        "\n",
        "train, validate, test = np.split(df.sample(frac=1, random_state=42),\n",
        "                       [int(.8*len(df)), int(.9*len(df))])"
      ],
      "metadata": {
        "id": "iPfMTt6eznQx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing Labels - Needs manual label"
      ],
      "metadata": {
        "id": "LThCDRYUzp5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting the pandas dataframe into a list of labels... \n",
        "#We may consider puting this into a function... \n",
        "\n",
        "trainlabel=train['label'].tolist()\n",
        "vallabel=validate['label'].tolist()\n",
        "testlabel=test['label'].tolist()"
      ],
      "metadata": {
        "id": "tYKZBDB3zs_n"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the classes variable... \n",
        "\n",
        "classes = list(fear_axis.keys())\n"
      ],
      "metadata": {
        "id": "6yqHv7PJ4bBe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map each class to a unique integer\n",
        "classes_to_index = dict((c, i) for i, c in enumerate(classes))\n",
        "\n",
        "# Map each integer back to its corresponding class\n",
        "index_to_classes = dict((v, k) for k, v in classes_to_index.items())"
      ],
      "metadata": {
        "id": "IMs8eE1QzvLT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a lambda function...called \"names_to_ids\"\n",
        "\n",
        "names_to_ids = lambda labels: np.array([classes_to_index.get(x) for x in labels])"
      ],
      "metadata": {
        "id": "1JcN3l8o0EKH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying the names_to_ids functions to the labels\n",
        "\n",
        "train_labels = names_to_ids(trainlabel)\n",
        "val_labels = names_to_ids(vallabel)\n",
        "test_labels = names_to_ids(testlabel)\n",
        "\n",
        "#Testing out the labels...\n",
        "print(train_labels[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t86jjfI0FkB",
        "outputId": "8743bd2a-6ec5-4d00-960c-36d9259278c2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input Training Text and Tokenizing Tweets"
      ],
      "metadata": {
        "id": "yCIoamZJ0HU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting the different pandas dataframes into a list of text fields...\n",
        "#Choice of variables: truncated_text, text_minus1, 'filtered_text',''filtered_text_minus1'\n",
        "\n",
        "traintext=train['text'].tolist()\n",
        "valtext=validate['text'].tolist()\n",
        "testtext=test['text'].tolist()"
      ],
      "metadata": {
        "id": "ujG0RIpm0MAn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the tokenizer...\n",
        "#Input into the tokenizer is a list\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "r1ngFMxo0RIZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input into the tokenizer is a list\n",
        "\n",
        "tokenizer = Tokenizer(num_words=128945, oov_token='<UNK>')\n",
        "\n",
        "#I think that this is the missing piece...I'm not really sure what it does...  \n",
        "\n",
        "tokenizer.fit_on_texts(traintext)\n",
        "#tokenizer.fit_on_texts(valtext)\n",
        "\n",
        "#Testing the tokenization... \n",
        "\n",
        "#print(tokenizer.texts_to_sequences([tweets[10]]))"
      ],
      "metadata": {
        "id": "2vZwl0WF0TQ6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Padding and Truncating Sequences "
      ],
      "metadata": {
        "id": "c8HPMv5G0VVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "yb6j0LVO0XN0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function that tokenizes and pads the sequences...\n",
        "\n",
        "def get_sequences(tokenizer, texts):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=200, padding='post')\n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "0tx-hkxo0aXP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying the function to tokenize and pad... to all test,validate and test\n",
        "#Syntax: val_sequences = get_sequences(tokenizer, val_tweets)\n",
        "\n",
        "padded_train_sequences = get_sequences(tokenizer, traintext)\n",
        "val_sequence = get_sequences(tokenizer,valtext)\n",
        "test_sequence = get_sequences(tokenizer,testtext)\n"
      ],
      "metadata": {
        "id": "ZUm5oBbQ0chu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating and training the model "
      ],
      "metadata": {
        "id": "rl6l2FtS0ecg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding layer works on number of tokens -> approximately words... \n",
        "#Adding another layer: tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(500, return_sequences=True)),\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(128945, 100, input_length=200),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100)),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfqZq2Hw0iGL",
        "outputId": "b6b81013-ad66-4d36-fb96-8fe9f288c5e9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 200, 100)          12894500  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 200, 200)         160800    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 200)              240800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 1206      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,297,306\n",
            "Trainable params: 13,297,306\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Having the callbacks option on means that the model stops once you are like over-generalizing... \n",
        "#Let's leave the callback option on for now...\n",
        "\n",
        "\n",
        "h = model.fit(\n",
        "    padded_train_sequences, train_labels,\n",
        "    validation_data=(val_sequence, val_labels),\n",
        "    epochs=4,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)\n",
        "   ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Gu2xnW0nGa",
        "outputId": "11cd1c57-77ae-46d8-fd28-bf497bbe7c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "5385/6657 [=======================>......] - ETA: 51s - loss: 0.9525 - accuracy: 0.6431"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute class frequencies\n",
        "class_freqs = np.bincount(train_labels)\n",
        "total_samples = np.sum(class_freqs)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = {}\n",
        "for i, freq in enumerate(class_freqs):\n",
        "    class_weights[i] = total_samples / (len(class_freqs) * freq)\n",
        "\n"
      ],
      "metadata": {
        "id": "NlapgLWTtQ1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with class weights\n",
        "#h = model.fit(\n",
        "    padded_train_sequences, train_labels,\n",
        "    validation_data=(val_sequence, val_labels),\n",
        "    epochs=2,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)\n",
        "   ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "safXM8N7r1Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating The Model "
      ],
      "metadata": {
        "id": "h9vH4glT0pvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_history(h):\n",
        "    epochs_trained = len(h.history['loss'])\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')\n",
        "    plt.ylim([0., 1.])\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "show_history(h)"
      ],
      "metadata": {
        "id": "9D4Kl54K0rww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running the model on the test sequence and test labels... \n",
        "\n",
        "eval = model.evaluate(test_sequence, test_labels)"
      ],
      "metadata": {
        "id": "KKJGeWYD0uKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preds = model.predict_classes(test_sequence)\n",
        "preds=model.predict(test_sequence) \n",
        "classes_x=np.argmax(preds,axis=1)\n",
        "preds.shape, test_labels.shape"
      ],
      "metadata": {
        "id": "TVtCPVVV0wMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Error Analysis "
      ],
      "metadata": {
        "id": "mi85BmDv0xwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dumb luck metric \n",
        "\n",
        "counts = df['label'].value_counts()\n",
        "dumb_luck = max(counts) / sum(counts)\n",
        "\n",
        "dumb_luck\n"
      ],
      "metadata": {
        "id": "N0bPxr94oPwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an inference dataframe \n",
        "\n",
        "inf_df=pd.DataFrame({\n",
        "    'data':testtext,\n",
        "    \"labels_predicted\": classes_x                    \n",
        "})\n",
        "inf_df[\"labels_predicted_marked\"]=inf_df['labels_predicted'].apply(lambda x: index_to_classes[x])\n",
        "inf_df[\"actual_labels\"]=testlabel\n",
        "\n",
        "#Creating the labels index datastructure...\n",
        "\n",
        "inf_df[\"actual_label_index\"]=inf_df['actual_labels'].apply(lambda x: classes_to_index[x])\n",
        "\n"
      ],
      "metadata": {
        "id": "LtVkH2y70zda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the inf_df shape...\n",
        "\n",
        "inf_df.shape"
      ],
      "metadata": {
        "id": "frs6-UaJ05J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making the correct prediction \n",
        "\n",
        "inf_df['correct_pred']=0 # first assigning all to 0.\n",
        "inf_df.loc[(inf_df['labels_predicted']==inf_df['actual_label_index']),'correct_pred']=1 # labelling 1 if the prediction is right."
      ],
      "metadata": {
        "id": "djekeBFK06Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# magnitutde of error\n",
        "inf_df['error_magnitude']=abs(inf_df['labels_predicted']-inf_df['actual_label_index'])"
      ],
      "metadata": {
        "id": "Yc0hQQnc0829"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the frequency of each class in the error_magnitude column\n",
        "counts = inf_df['error_magnitude'].value_counts()\n",
        "\n",
        "# Create a histogram with one bar for each class\n",
        "plt.bar(counts.index, counts.values)\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.suptitle('Histogram of Error Magnitude in Fear Classification')\n",
        "plt.xlabel('Magnitude')\n",
        "plt.ylabel('Frequency')"
      ],
      "metadata": {
        "id": "RKRJBQcf0-X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving the model and evaluation dataset"
      ],
      "metadata": {
        "id": "fWaZuOGY1CFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#So this works but we need to make sure that we install Keras as a dependency \n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "#model.save('/content/drive/MyDrive/Sean/Emoclass_Dec2022/Model_Fear_151K_Feb23,2022')"
      ],
      "metadata": {
        "id": "WR0pNn031Gez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting the evaluation dataset "
      ],
      "metadata": {
        "id": "d2OS1oKQ1KVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting the dataset \n",
        "\n",
        "#path = \"/content/drive/MyDrive/Sean/Emoclass/Train_Fear_Error_Analysis_151K_March8,2023.csv\"\n",
        "inf_df.to_csv(path)\n"
      ],
      "metadata": {
        "id": "vTNJKbfw1NDi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}